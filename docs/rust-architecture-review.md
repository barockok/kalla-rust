# Kalla Rust Architecture Review

> **Date:** 2026-02-19
> **Scope:** All 6 Rust crates in the workspace
> **Focus:** API schema clarity, internal contract definition, connector configuration handling

---

## Table of Contents

- [System Overview](#system-overview)
- [Crate Map](#crate-map)
- [API Surface](#api-surface)
  - [HTTP Endpoints (Runner)](#http-endpoints-runner)
  - [Callback Protocol (Outbound)](#callback-protocol-outbound)
  - [Rust Library APIs](#rust-library-apis)
- [Connector Configuration Model](#connector-configuration-model)
- [Data Flow](#data-flow)
- [Findings](#findings)
  - [Blockers](#blockers)
  - [Design Issues](#design-issues)
  - [Suggestions](#suggestions)
- [Summary Table](#summary-table)
- [Recommended Fix Order](#recommended-fix-order)

---

## System Overview

Kalla is a financial reconciliation engine. It takes two data sources (left + right), runs SQL-based matching via Apache DataFusion, and produces matched/unmatched evidence records in Parquet format.

**What the Rust layer provides:**

- A reconciliation engine wrapping DataFusion with financial UDFs
- Connectors for PostgreSQL, S3, and local files
- An HTTP job runner that accepts reconciliation jobs, executes them asynchronously, and reports results via callbacks
- An evidence store that writes matched/unmatched records to Parquet files
- Optional distributed execution via Apache Ballista

**What the Rust layer does NOT provide:**

- Recipe storage/management (handled by the API layer)
- User authentication or authorization
- A public-facing API (all endpoints are internal, consumed by the API layer)

**The only persistent data the Rust layer reads is connector configuration** (connection strings, S3 credentials, file paths) provided at job submission time. It does not maintain its own persistent state.

---

## Crate Map

```
kalla (workspace)
├── kalla-core          Reconciliation engine (DataFusion wrapper + UDFs)
├── kalla-connectors    Data source adapters (Postgres, S3, CSV, BigQuery stub)
├── kalla-recipe        Recipe schema definitions (serialization only)
├── kalla-evidence      Evidence store (Parquet writer for results)
├── kalla-ballista      Ballista integration + HTTP job runner
└── kallad              Unified daemon binary (CLI entry point)
```

### Dependency Graph

```
kallad
 └── kalla-ballista
      ├── kalla-core
      │    └── datafusion, arrow
      ├── kalla-connectors
      │    ├── kalla-core (for SessionContext)
      │    ├── sqlx (postgres)
      │    └── object_store (S3)
      └── kalla-evidence
           └── parquet, arrow
```

`kalla-recipe` is standalone (no internal dependencies). It defines serialization types only.

---

## API Surface

### HTTP Endpoints (Runner)

The HTTP runner is embedded inside the Ballista scheduler process. It provides 4 endpoints:

| Endpoint | Method | Status | Purpose |
|---|---|---|---|
| `/api/jobs` | POST | 202 Accepted | Submit a reconciliation job for async execution |
| `/health` | GET | 200 | Liveness probe |
| `/ready` | GET | 200 | Readiness probe |
| `/metrics` | GET | 200 | Prometheus metrics (text format) |

#### POST /api/jobs

**Request body (`JobRequest`):**

```rust
// kalla-ballista/src/runner.rs:36-43
pub struct JobRequest {
    pub run_id: Uuid,
    pub callback_url: String,
    pub match_sql: String,
    pub sources: Vec<ResolvedSource>,
    pub output_path: String,
    pub primary_keys: HashMap<String, Vec<String>>,
}
```

| Field | Type | Description |
|---|---|---|
| `run_id` | UUID | Unique identifier for this run (generated by caller) |
| `callback_url` | String | Base URL for progress/completion/error callbacks |
| `match_sql` | String | Raw DataFusion SQL that produces matched records |
| `sources` | Vec | Data sources to register before executing SQL |
| `output_path` | String | Base path for output Parquet files |
| `primary_keys` | HashMap | Mapping of source alias to primary key column names |

**Source definition (`ResolvedSource`):**

```rust
// kalla-ballista/src/runner.rs:46-50
pub struct ResolvedSource {
    pub alias: String,  // Table alias used in match_sql
    pub uri: String,    // Connection URI (postgres://, s3://, or file path)
}
```

**Response body (`JobAccepted`):**

```rust
// kalla-ballista/src/runner.rs:53-57
pub struct JobAccepted {
    pub run_id: Uuid,
    pub status: String,  // Always "accepted"
}
```

#### GET /metrics

Returns Prometheus text format with these metrics:

| Metric | Type | Description |
|---|---|---|
| `kalla_runner_active_jobs` | Gauge | Number of jobs currently executing |
| `kalla_runner_jobs_completed` | Counter | Total jobs completed successfully |
| `kalla_runner_jobs_failed` | Counter | Total jobs that failed |

---

### Callback Protocol (Outbound)

The runner reports job progress by making HTTP POST requests to the `callback_url` provided in the `JobRequest`. Three callback types exist:

#### Progress Callback

**Endpoint:** `POST {callback_url}/progress`

Sent during staging (source registration) and matching phases.

```json
{
  "run_id": "uuid",
  "stage": "staging | matching",
  "progress": 0.5,
  "source": "invoices",
  "matched_count": 1000
}
```

> **Note:** The fields present vary by stage. During `staging`, `source` and `progress` (0.0-1.0) are included. During `matching`, `matched_count` is included instead of `progress`.

#### Completion Callback

**Endpoint:** `POST {callback_url}/complete`

Sent once when the job finishes successfully.

```json
{
  "run_id": "uuid",
  "matched_count": 4500,
  "unmatched_left_count": 150,
  "unmatched_right_count": 75,
  "output_paths": {
    "matched": "{output_path}/matched.parquet",
    "unmatched_left": "{output_path}/unmatched_left.parquet",
    "unmatched_right": "{output_path}/unmatched_right.parquet"
  }
}
```

#### Error Callback

**Endpoint:** `POST {callback_url}/error`

Sent when the job fails.

```json
{
  "run_id": "uuid",
  "error": "Error message string",
  "stage": "matching"
}
```

> **Note:** The `stage` field is only present for errors during matching. Top-level failures omit it.

---

### Rust Library APIs

#### ReconciliationEngine (`kalla-core`)

```
ReconciliationEngine::new()                    -> Self               // Local mode
ReconciliationEngine::new_distributed()        -> Result<Self>       // Ballista standalone
ReconciliationEngine::new_cluster(url, ...)    -> Result<Self>       // Ballista cluster

engine.context()                               -> &SessionContext    // Direct DataFusion access
engine.context_mut()                           -> &mut SessionContext

engine.register_csv(name, path)                -> DFResult<()>
engine.register_parquet(name, path)            -> DFResult<()>

engine.sql(query)                              -> DFResult<DataFrame>
engine.sql_stream(query)                       -> DFResult<SendableRecordBatchStream>

engine.execute_join(left, right, conditions)   -> DFResult<DataFrame>
engine.execute_join_stream(...)                -> DFResult<SendableRecordBatchStream>

engine.find_left_orphans(left, right, lk, rk)  -> DFResult<DataFrame>
engine.find_left_orphans_stream(...)           -> DFResult<SendableRecordBatchStream>
engine.find_right_orphans(...)                 -> DFResult<DataFrame>
engine.find_right_orphans_stream(...)          -> DFResult<SendableRecordBatchStream>
```

**Financial UDFs registered automatically:**

| UDF | Signature | Description |
|---|---|---|
| `tolerance_match` | `(f64, f64, f64) -> bool` | Returns `true` if `abs(a - b) <= threshold` |

#### SourceConnector Trait (`kalla-connectors`)

```rust
#[async_trait]
pub trait SourceConnector: Send + Sync {
    async fn register_table(ctx, table_name, source_table, where_clause) -> Result<()>;
    async fn register_scoped(ctx, table_name, source_table, conditions, limit) -> Result<usize>;
    async fn stream_table(ctx, table_name) -> Result<SendableRecordBatchStream>;
}
```

**Implementations:**

| Connector | Struct | Creates via |
|---|---|---|
| PostgreSQL | `PostgresConnector` | `PostgresConnector::new(connection_string)` |
| PostgreSQL (partitioned) | `PostgresPartitionedTable` | `PostgresPartitionedTable::new(conn, table, partitions, order_col)` |
| S3 (Parquet/CSV) | `S3Connector` | `S3Connector::new(S3Config)` or `S3Connector::from_env()` |
| S3 CSV (partitioned) | `CsvByteRangeTable` | `csv_partitioned::register(ctx, alias, uri, partitions, config)` |
| BigQuery | `BigQueryConnector` | Stub only, all methods return "not yet implemented" |

#### EvidenceStore (`kalla-evidence`)

```
EvidenceStore::new(base_path)                  -> Result<Self>

store.init_run(metadata)                       -> Result<PathBuf>
store.update_metadata(metadata)                -> Result<()>
store.write_matched(run_id, records)           -> Result<PathBuf>
store.write_unmatched(run_id, records, side)   -> Result<PathBuf>
store.latest_run()                             -> Result<Option<PathBuf>>
```

**Output directory structure:**

```
{base_path}/runs/{run_id}/
  ├── metadata.json
  ├── matched.parquet
  ├── unmatched_left.parquet
  └── unmatched_right.parquet
```

#### Recipe Schema (`kalla-recipe`)

Serialization-only types (no logic beyond validation):

```rust
pub struct Recipe {
    pub recipe_id: String,
    pub name: String,
    pub description: String,
    pub match_sql: String,              // Raw DataFusion SQL
    pub match_description: String,      // Human-readable explanation
    pub sources: RecipeSources,
}

pub struct RecipeSources {
    pub left: RecipeSource,
    pub right: RecipeSource,
}

pub struct RecipeSource {
    pub alias: String,
    pub source_type: SourceType,        // postgres | bigquery | elasticsearch | file
    pub uri: Option<String>,            // None for file sources
    pub schema: Option<Vec<String>>,    // Column names for file sources
    pub primary_key: Vec<String>,
}
```

---

## Connector Configuration Model

The Rust layer reads connector configuration **only at job submission time**, via the `ResolvedSource.uri` field in `JobRequest`. There is no persistent config store in Rust.

### URI-Based Connector Resolution

The runner (`register_source_partitioned` in `runner.rs:260-322`) selects the connector implementation by pattern-matching the URI:

| URI Pattern | Connector Used | Config Source |
|---|---|---|
| `postgres://...` or `postgresql://...` | `PostgresPartitionedTable` | Connection string from URI, table name from `?table=` query param |
| `s3://*.csv` | `CsvByteRangeTable` | S3 credentials from environment variables |
| `s3://*` (non-CSV) | `S3Connector` | S3 credentials from environment variables |
| `*.csv` (local path) | `engine.register_csv()` | File path from URI |
| `*.parquet` or `*/staging/*` | `engine.register_parquet()` | File path from URI |

### PostgreSQL Configuration

- **Input:** Full connection string as URI (e.g., `postgres://user:pass@host:5432/db?table=invoices`)
- **Table name:** Extracted from `?table=` query parameter (required, error if missing)
- **Connection pooling:** `PgPoolOptions::new().max_connections(5)`
- **Partitioning:** Uses `ctid` (PostgreSQL row identifier) as the default order column for distributed reads

### S3 Configuration

- **Input:** S3 URI (e.g., `s3://bucket/path/to/file.parquet`)
- **Credentials:** Read from environment variables at runtime:

| Env Variable | Required | Default |
|---|---|---|
| `AWS_ACCESS_KEY_ID` | Yes | — |
| `AWS_SECRET_ACCESS_KEY` | Yes | — |
| `AWS_REGION` | No | `us-east-1` |
| `AWS_ENDPOINT_URL` | No | AWS default |
| `AWS_ALLOW_HTTP` | No | `false` |

**`S3Config` struct:**

```rust
pub struct S3Config {
    pub region: String,
    pub access_key_id: String,
    pub secret_access_key: String,
    pub endpoint_url: Option<String>,   // For MinIO/LocalStack
    pub allow_http: bool,
}
```

### File Configuration

Local files have no configuration beyond the file path. CSV and Parquet formats are auto-detected by file extension.

---

## Data Flow

```
                    API Layer (Node.js / external)
                           │
                           │  POST /api/jobs (JobRequest)
                           ▼
                    ┌──────────────┐
                    │  HTTP Runner │  (kalla-ballista/runner.rs)
                    └──────┬───────┘
                           │
              ┌────────────┼────────────┐
              ▼            ▼            ▼
        ┌──────────┐ ┌──────────┐ ┌──────────┐
        │ Register │ │ Register │ │ Register │   Source registration
        │ Source A │ │ Source B │ │ Source N │   (connectors crate)
        └────┬─────┘ └────┬─────┘ └────┬─────┘
             │             │            │
             └─────────┬───┘────────────┘
                       ▼
              ┌────────────────┐
              │  DataFusion    │   Execute match_sql
              │  Engine        │   (kalla-core)
              └───────┬────────┘
                      │
                      ▼
              ┌────────────────┐
              │ Evidence Store │   Write Parquet files
              │                │   (kalla-evidence)
              └───────┬────────┘
                      │
                      ▼
              POST {callback_url}/complete
                      │
                      ▼
                 API Layer receives results
```

**Execution modes:**

1. **Local mode:** DataFusion runs queries in-process (default fallback)
2. **Cluster mode:** DataFusion distributes queries via Ballista scheduler/executors (preferred when executors are connected)

The runner automatically probes for executors with a `SELECT 1` query (10-second timeout) and falls back to local mode if no executors respond.

---

## Findings

### Blockers

Issues that will cause production failures under real load.

#### B1. Callback payloads are untyped `serde_json::Value`

**Location:** `kalla-ballista/src/runner.rs:82-119` (CallbackClient methods), `runner.rs:551-737` (inline json! macros)

**Problem:** All three callback types (progress, complete, error) construct their payloads as inline `serde_json::json!()` macros. There is no struct defining the callback contract. The caller (API layer) must reverse-engineer the shape from reading Rust source code.

Additionally, the progress callback shape varies depending on the stage — during `staging` it includes `source` and `progress`, during `matching` it includes `matched_count` instead. This variability is implicit.

**Impact:** Contract drift between the Rust runner and the API layer. If a field is renamed or a new field is added, there's no compile-time check.

**Recommended fix:** Define typed structs for each callback payload:

```rust
#[derive(Debug, Serialize)]
#[serde(tag = "stage")]
pub enum ProgressCallback {
    #[serde(rename = "staging")]
    Staging {
        run_id: Uuid,
        source: Option<String>,
        progress: f64,
    },
    #[serde(rename = "matching")]
    Matching {
        run_id: Uuid,
        matched_count: u64,
    },
}

#[derive(Debug, Serialize)]
pub struct CompletionCallback {
    pub run_id: Uuid,
    pub matched_count: u64,
    pub unmatched_left_count: u64,
    pub unmatched_right_count: u64,
    pub output_paths: OutputPaths,
}

#[derive(Debug, Serialize)]
pub struct OutputPaths {
    pub matched: String,
    pub unmatched_left: String,
    pub unmatched_right: String,
}

#[derive(Debug, Serialize)]
pub struct ErrorCallback {
    pub run_id: Uuid,
    pub error: String,
    pub stage: Option<String>,
}
```

---

#### B2. CallbackClient has no timeout, retry, or error handling

**Location:** `kalla-ballista/src/runner.rs:70-126`

**Problem:** The `reqwest::Client` is created with default settings (no timeout). Every callback is fire-and-forget:

- `report_progress` blocks the job execution loop if the callback endpoint is slow
- `report_complete` silently fails if the endpoint is unreachable — the caller never learns the job finished
- No retry policy, no circuit breaker, no backoff

**Impact:** A slow or unavailable callback endpoint can stall or silently drop job completion notifications.

**Recommended fix:**

```rust
impl CallbackClient {
    pub fn new() -> Self {
        Self {
            http: reqwest::Client::builder()
                .timeout(Duration::from_secs(10))
                .connect_timeout(Duration::from_secs(5))
                .build()
                .unwrap(),
        }
    }
}
```

For `report_complete`, add retry with exponential backoff (this is the most critical callback — if it fails, the caller never knows the job finished). Progress callbacks can remain best-effort.

---

#### B3. No concurrency limit on job execution

**Location:** `kalla-ballista/src/runner.rs:781-787`

**Problem:** Every incoming job spawns an unbounded `tokio::spawn`:

```rust
while let Some(job) = job_rx.recv().await {
    let cfg = config.clone();
    let m = runner_metrics.clone();
    tokio::spawn(async move {
        execute_job(job, &cfg, &m).await;
    });
}
```

Under load, N simultaneous jobs will each:
- Create a DataFusion engine (memory allocation)
- Load sources into memory (for PostgreSQL `MemTable` path)
- Execute queries (CPU and memory)

No upper bound means the system can OOM or exhaust CPU.

**Impact:** Resource exhaustion under concurrent load.

**Recommended fix:** Use a semaphore to bound concurrency:

```rust
let semaphore = Arc::new(Semaphore::new(config.max_concurrent_jobs));

while let Some(job) = job_rx.recv().await {
    let permit = semaphore.clone().acquire_owned().await.unwrap();
    let cfg = config.clone();
    let m = runner_metrics.clone();
    tokio::spawn(async move {
        execute_job(job, &cfg, &m).await;
        drop(permit);
    });
}
```

Also add a `kalla_runner_queued_jobs` gauge metric for observability.

---

#### B4. Readiness probe always returns 200

**Location:** `kalla-ballista/src/runner.rs:244-246`

**Problem:** The `/ready` endpoint unconditionally returns `StatusCode::OK`:

```rust
async fn ready() -> StatusCode {
    StatusCode::OK
}
```

It does not verify whether the system can actually accept work (e.g., is the job channel full? is the scheduler reachable?).

**Impact:** Kubernetes will route traffic to a pod that cannot accept jobs, leading to silent failures.

**Recommended fix:**

```rust
async fn ready(State(state): State<Arc<RunnerState>>) -> StatusCode {
    if state.job_tx.capacity() > 0 {
        StatusCode::OK
    } else {
        StatusCode::SERVICE_UNAVAILABLE
    }
}
```

---

### Design Issues

Issues that affect maintainability and extensibility.

#### D1. SourceConnector trait is not used polymorphically

**Location:** `kalla-connectors/src/lib.rs:28-55` (trait definition) vs `kalla-ballista/src/runner.rs:260-322` (actual usage)

**Problem:** The `SourceConnector` trait defines a clean interface with `register_table`, `register_scoped`, and `stream_table`. However, the runner completely ignores this trait. Instead, `register_source_partitioned` uses URI-prefix string matching to select the concrete connector:

```rust
if uri.starts_with("postgres://") || uri.starts_with("postgresql://") {
    // Direct PostgresPartitionedTable construction
} else if uri.starts_with("s3://") && uri.ends_with(".csv") {
    // Direct csv_partitioned::register call
} else if uri.starts_with("s3://") {
    // Direct S3Connector construction
} else if uri.ends_with(".csv") {
    // Direct engine.register_csv
} ...
```

This is a switch-on-type anti-pattern. Adding a new connector type (BigQuery, Elasticsearch) requires modifying this function.

**Recommended fix:** Define a `ConnectorFactory` trait or registry:

```rust
pub trait ConnectorFactory: Send + Sync {
    fn can_handle(&self, uri: &str) -> bool;
    async fn register(
        &self,
        ctx: &SessionContext,
        alias: &str,
        uri: &str,
        partitions: usize,
    ) -> Result<u64>;
}
```

Register factories at startup; iterate to find the handler. New connectors become additive (Open/Closed principle).

---

#### D2. Recipe and JobRequest are disconnected contracts

**Location:** `kalla-recipe/src/schema.rs` (Recipe, RecipeSource, RecipeSources) vs `kalla-ballista/src/runner.rs:36-57` (JobRequest, ResolvedSource)

**Problem:** Two parallel representations of the same concept exist with no formal mapping:

| Concept | Recipe | JobRequest |
|---|---|---|
| Sources | `RecipeSources { left, right }` | `Vec<ResolvedSource>` |
| Source type | `source_type: SourceType` enum | Inferred from URI pattern |
| Primary keys | `primary_key: Vec<String>` per source | `primary_keys: HashMap<String, Vec<String>>` |
| Match logic | `match_sql: String` | `match_sql: String` |
| Source identity | `alias + uri + schema` | `alias + uri` |

The translation between `Recipe` and `JobRequest` happens in the API layer (not Rust). This means the contract is implicit — if Recipe changes, there's no compile-time signal that JobRequest needs updating.

**Recommended fix:** Either:
- Add `impl From<Recipe> for JobRequest` in Rust (makes the mapping explicit and testable), or
- Have `JobRequest` embed or reference `Recipe` directly

---

#### D3. EvidenceStore::write_unmatched uses stringly-typed `side` parameter

**Location:** `kalla-evidence/src/store.rs:122`

**Problem:**

```rust
pub fn write_unmatched(&self, run_id: &Uuid, records: &[UnmatchedRecord], side: &str) -> Result<PathBuf>
```

`side` accepts arbitrary strings. Passing `"LEFT"` instead of `"left"` produces `unmatched_LEFT.parquet` silently.

**Recommended fix:**

```rust
pub enum Side {
    Left,
    Right,
}

impl Side {
    fn as_str(&self) -> &str {
        match self {
            Side::Left => "left",
            Side::Right => "right",
        }
    }
}
```

---

#### D4. ReconciliationEngine leaks its internals via context() / context_mut()

**Location:** `kalla-core/src/engine.rs:93-99`

**Problem:**

```rust
pub fn context(&self) -> &SessionContext { &self.ctx }
pub fn context_mut(&mut self) -> &mut SessionContext { &mut self.ctx }
```

Any consumer can bypass the engine's abstraction and interact with DataFusion directly. The runner does this extensively (e.g., `engine.context().register_table(...)`).

This means `ReconciliationEngine` is not a real abstraction boundary — it's a thin wrapper that's trivially bypassed. The engine provides methods like `register_csv`, `execute_join`, `find_left_orphans`, but these are undercut by direct context access.

**Recommended fix:** Choose one:
- **Commit to the abstraction:** Add `register_table(name, Arc<dyn TableProvider>)` to the engine and remove `context()` / `context_mut()`. All registration goes through the engine.
- **Drop the wrapper:** If the engine is just `SessionContext + UDFs`, make that explicit. Provide a `create_session() -> SessionContext` factory function instead of a wrapper struct.

---

#### D5. No custom error types anywhere

**Location:** All crates

**Problem:** Every crate uses `anyhow::Result` (or `DFResult` which is `datafusion::common::Result`). There are no custom error enums. Callers can only inspect error messages as strings.

**Impact:** The API layer cannot distinguish between "connection refused" and "schema mismatch" and "SQL syntax error" without parsing error message strings. This makes error handling brittle and makes it impossible to return structured error responses to users.

**Recommended fix:** Define error enums at crate boundaries:

```rust
// kalla-connectors
pub enum ConnectorError {
    ConnectionFailed { uri: String, cause: String },
    TableNotFound { table: String },
    SchemaMismatch { expected: Vec<String>, got: Vec<String> },
    UnsupportedUri { uri: String },
}

// kalla-ballista
pub enum RunnerError {
    SourceRegistrationFailed { alias: String, cause: ConnectorError },
    MatchSqlFailed { sql: String, cause: String },
    EvidenceWriteFailed { run_id: Uuid, cause: String },
    CallbackFailed { url: String, cause: String },
}
```

---

#### D6. SQL format-string interpolation in engine methods

**Location:** `kalla-core/src/engine.rs:130-222`

**Problem:** `execute_join`, `find_left_orphans`, `find_right_orphans` all build SQL via `format!()`:

```rust
let query = format!(
    "SELECT * FROM {} AS l INNER JOIN {} AS r ON {}",
    left_table, right_table, join_conditions
);
```

`left_table`, `right_table`, and `join_conditions` are interpolated directly. Since this is internal and the caller controls these values, the risk is lower. However, table names are not quoted, which means names containing spaces or SQL keywords will break.

**Recommended fix:** Quote table identifiers:

```rust
let query = format!(
    "SELECT * FROM \"{}\" AS l INNER JOIN \"{}\" AS r ON {}",
    left_table, right_table, join_conditions
);
```

The `match_sql` field in Recipe is raw SQL by design — this is an accepted trust boundary, but should be documented explicitly.

---

### Suggestions

Polish items that improve quality but aren't blockers.

#### S1. S3Config derives Serialize with secrets in plain fields

**Location:** `kalla-connectors/src/s3.rs:26-38`

**Problem:** `S3Config` has `access_key_id` and `secret_access_key` as plain public fields with `#[derive(Serialize, Debug)]`. If this struct is ever logged or serialized for debugging, credentials leak.

**Recommended fix:**

```rust
#[derive(Debug, Clone, Deserialize)]
pub struct S3Config {
    pub region: String,
    #[serde(skip_serializing)]
    pub access_key_id: String,
    #[serde(skip_serializing)]
    pub secret_access_key: String,
    pub endpoint_url: Option<String>,
    pub allow_http: bool,
}
```

Or implement `Debug` manually to redact secrets.

---

#### S2. extract_string_value only handles String and Int64

**Location:** `kalla-ballista/src/runner.rs:406-419`

**Problem:** The key extraction function only handles `StringArray` and `Int64Array`. If a primary key column is `Int32`, `Float64`, or another type, it returns `None`, and the matched record gets a synthetic key like `row_0`.

This silently corrupts the unmatched counting logic — distinct matched key sets will be smaller than actual, causing inflated unmatched counts.

**Recommended fix:** Handle all numeric types, or use Arrow's `display` formatting to convert any array element to string.

---

#### S3. RunnerConfig and SchedulerOpts share redundant fields

**Location:** `kalla-ballista/src/runner.rs:194-201` (RunnerConfig) vs `kalla-ballista/src/lib.rs` (SchedulerOpts)

**Problem:** Both structs have `grpc_port`, `staging_path`, and `partitions`. The scheduler constructs a `RunnerConfig` from its own `SchedulerOpts`. Consider collapsing into a single config type.

---

#### S4. PostgreSQL connector loads entire table into memory

**Location:** `kalla-connectors/src/postgres.rs:39-92`

**Problem:** `PostgresConnector::register_table` fetches all rows via `fetch_all` and loads them into a `MemTable`. For large tables (millions of rows), this will OOM.

The partitioned path (`PostgresPartitionedTable`) avoids this by streaming, but the non-partitioned `SourceConnector` trait implementation still loads everything.

**Note:** This is partially mitigated because the runner uses `PostgresPartitionedTable` by default. But the `SourceConnector` trait implementation on `PostgresConnector` is still dangerous if used directly.

---

## Summary Table

| # | Issue | Severity | Category | Location |
|---|---|---|---|---|
| B1 | Untyped callback payloads | **Blocker** | API contract | `runner.rs` |
| B2 | No timeout/retry on callbacks | **Blocker** | Resilience | `runner.rs:70-126` |
| B3 | Unbounded job concurrency | **Blocker** | Resource safety | `runner.rs:781-787` |
| B4 | Readiness probe always true | **Blocker** | Operations | `runner.rs:244-246` |
| D1 | SourceConnector not used polymorphically | Design | Open/Closed | `runner.rs:260-322` |
| D2 | Recipe / JobRequest contract gap | Design | API contract | `recipe/schema.rs`, `runner.rs` |
| D3 | Stringly-typed `side` parameter | Design | Type safety | `store.rs:122` |
| D4 | Engine leaks SessionContext | Design | Abstraction | `engine.rs:93-99` |
| D5 | No custom error types | Design | Error handling | All crates |
| D6 | SQL format-string without quoting | Design | Safety | `engine.rs:130-222` |
| S1 | Serializable secrets in S3Config | Suggestion | Security | `s3.rs:26-38` |
| S2 | Incomplete key type extraction | Suggestion | Correctness | `runner.rs:406-419` |
| S3 | Config struct redundancy | Suggestion | DRY | `runner.rs`, `lib.rs` |
| S4 | PostgresConnector loads full table | Suggestion | Memory safety | `postgres.rs:39-92` |

---

## Recommended Fix Order

1. **B1 + B2** (Callback contract) — Define typed callback structs and add timeout/retry. These are the API contract that the Node.js layer depends on.
2. **B3** (Concurrency limit) — Add semaphore. Without this, any concurrent load will OOM.
3. **B4** (Ready probe) — Quick fix, high operational impact.
4. **D3** (Side enum) — Quick fix, prevents silent bugs.
5. **D5** (Error types) — Foundation for structured error handling across the stack.
6. **D1** (Connector factory) — Required before adding new connector types (BigQuery, Elasticsearch).
7. **D2** (Recipe/JobRequest mapping) — Required before the Recipe schema evolves further.
8. **S1-S4** — Address during normal development.
